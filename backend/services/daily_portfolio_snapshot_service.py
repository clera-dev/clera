"""
Daily Portfolio Snapshot Service - Phase 2

Production-grade service for capturing end-of-day portfolio values.
Runs after market close to extend historical timelines forward.

Much lighter than Phase 1 reconstruction since it only captures current data.
Designed to handle millions of users efficiently with batch processing.

Key Features:
- Batch processing of all aggregation users
- Controlled concurrency to manage API load
- Comprehensive error handling and retry logic
- Performance monitoring and cost tracking
- Seamless timeline extension from reconstruction data
"""

import asyncio
import logging
import json
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, date, timedelta, time
from dataclasses import dataclass
import pytz

logger = logging.getLogger(__name__)

@dataclass
class EODSnapshot:
    """End-of-day portfolio snapshot."""
    user_id: str
    snapshot_date: date
    total_value: float
    total_cost_basis: float
    total_gain_loss: float
    total_gain_loss_percent: float
    account_breakdown: Dict[str, float]
    institution_breakdown: Dict[str, float]
    securities_count: int
    data_quality_score: float

@dataclass
class EODBatchResult:
    """Result of end-of-day batch processing."""
    total_users_processed: int
    successful_snapshots: int
    failed_snapshots: int
    total_portfolio_value: float
    processing_duration_seconds: float
    api_calls_made: int
    errors: List[str]

class DailyPortfolioSnapshotService:
    """
    Production-grade daily portfolio snapshot service.
    
    Captures end-of-day portfolio values for all aggregation users
    to extend their historical timelines forward from reconstruction data.
    """
    
    def __init__(self):
        """Initialize the daily snapshot service."""
        self.portfolio_service = None  # Lazy loaded
        self.supabase = None  # Lazy loaded
        
        # Configuration
        self.batch_size = 1000  # Users per batch
        self.max_concurrent = 20  # Concurrent operations per batch
        self.retry_attempts = 3
        
        # Performance tracking
        self.total_snapshots_created = 0
        self.total_api_calls = 0
        self.total_processing_time = 0.0
    
    def _get_services(self):
        """Lazy load required services."""
        if self.portfolio_service is None:
            from utils.portfolio.portfolio_service import get_portfolio_service
            self.portfolio_service = get_portfolio_service()
    
    def _get_supabase_client(self):
        """
        Lazy load Supabase client.
        ARCHITECTURE FIX: This method was missing, causing AttributeError at runtime.
        """
        if self.supabase is None:
            from utils.supabase.db_client import get_supabase_client
            self.supabase = get_supabase_client()
        return self.supabase
    
    async def capture_all_users_eod_snapshots(self, sync_stale_holdings: bool = True) -> EODBatchResult:
        """
        Capture EOD snapshots for all aggregation users.
        
        Args:
            sync_stale_holdings: If True, syncs stale SnapTrade holdings BEFORE capturing snapshots.
                                This ensures snapshots use fresh holdings quantities.
        
        Returns:
            EODBatchResult with comprehensive stats
        """
        # PRODUCTION-GRADE: Sync stale holdings BEFORE capturing snapshots
        # This ensures we're using fresh holdings quantities, not stale data
        if sync_stale_holdings:
            from services.smart_snaptrade_sync_service import get_smart_sync_service
            
            logger.info("ðŸ”„ Step 1/2: Syncing stale SnapTrade holdings...")
            sync_service = get_smart_sync_service(
                staleness_threshold_hours=23,  # Sync if not synced in last 23h
                batch_size=10,
                rate_limit_delay=0.5
            )
            
            sync_metrics = await sync_service.sync_stale_users(force=False)
            
            logger.info(
                f"âœ… Holdings sync complete: {sync_metrics.synced_users} users synced, "
                f"{sync_metrics.failed_users} failed, ${sync_metrics.estimated_cost_usd:.4f} cost"
            )
        
        # STEP 2: Now capture snapshots with fresh holdings
        logger.info("ðŸ“¸ Step 2/2: Capturing EOD snapshots...")
        
        start_time = datetime.now()
        
        try:
            self._get_services()
            
            logger.info("ðŸŒ… Starting daily EOD snapshot collection")
            
            # Get all aggregation mode users
            aggregation_users = await self._get_all_aggregation_users()
            logger.info(f"ðŸ“Š Found {len(aggregation_users)} aggregation users")
            
            if not aggregation_users:
                return EODBatchResult(
                    total_users_processed=0,
                    successful_snapshots=0,
                    failed_snapshots=0,
                    total_portfolio_value=0.0,
                    processing_duration_seconds=0.0,
                    api_calls_made=0,
                    errors=[]
                )
            
            # Process users in batches for memory efficiency
            all_results = []
            total_successful = 0
            total_failed = 0
            total_portfolio_value = 0.0
            all_errors = []
            
            user_batches = [aggregation_users[i:i+self.batch_size] 
                           for i in range(0, len(aggregation_users), self.batch_size)]
            
            for batch_num, user_batch in enumerate(user_batches, 1):
                logger.info(f"ðŸ”„ Processing EOD batch {batch_num}/{len(user_batches)}: {len(user_batch)} users")
                
                # Process batch with controlled concurrency
                batch_results = await self._process_eod_batch(user_batch)
                
                # Aggregate results
                batch_successful = sum(1 for r in batch_results if r.get('success'))
                batch_failed = len(batch_results) - batch_successful
                batch_portfolio_value = sum(r.get('portfolio_value', 0) for r in batch_results if r.get('success'))
                batch_errors = [r.get('error') for r in batch_results if r.get('error')]
                
                total_successful += batch_successful
                total_failed += batch_failed
                total_portfolio_value += batch_portfolio_value
                all_errors.extend(batch_errors)
                
                logger.info(f"ðŸ“ˆ Batch {batch_num} complete: {batch_successful} successful, {batch_failed} failed")
                
                # Rate limiting between batches
                await asyncio.sleep(2)
            
            duration = (datetime.now() - start_time).total_seconds()
            
            # Update service metrics
            self.total_snapshots_created += total_successful
            self.total_processing_time += duration
            
            result = EODBatchResult(
                total_users_processed=len(aggregation_users),
                successful_snapshots=total_successful,
                failed_snapshots=total_failed,
                total_portfolio_value=total_portfolio_value,
                processing_duration_seconds=duration,
                api_calls_made=self.total_api_calls,
                errors=all_errors
            )
            
            logger.info(f"âœ… Daily EOD collection complete: {total_successful}/{len(aggregation_users)} users, "
                       f"${total_portfolio_value:,.2f} total value, {duration:.1f}s")
            
            # Store collection metrics for monitoring
            await self._store_collection_metrics(result)
            
            return result
            
        except Exception as e:
            duration = (datetime.now() - start_time).total_seconds()
            logger.error(f"âŒ Daily EOD collection failed after {duration:.1f}s: {e}")
            
            return EODBatchResult(
                total_users_processed=0,
                successful_snapshots=0,
                failed_snapshots=0,
                total_portfolio_value=0.0,
                processing_duration_seconds=duration,
                api_calls_made=0,
                errors=[str(e)]
            )
    
    async def _get_all_aggregation_users(self) -> List[str]:
        """
        Get all users in aggregation mode who need daily snapshots.
        
        Includes users with:
        - Active Plaid connections (legacy)
        - Active SnapTrade connections (current)
        """
        try:
            supabase = self._get_supabase_client()
            
            # Get all users with active aggregation connections (Plaid OR SnapTrade)
            result = supabase.table('user_investment_accounts')\
                .select('user_id')\
                .in_('provider', ['plaid', 'snaptrade'])\
                .eq('is_active', True)\
                .execute()
            
            if result.data:
                user_ids = list(set(row['user_id'] for row in result.data))
                logger.info(f"ðŸ“Š Found {len(user_ids)} aggregation users (Plaid + SnapTrade)")
                return user_ids
            
            return []
            
        except Exception as e:
            logger.error(f"Error getting aggregation users: {e}")
            return []
    
    async def _process_eod_batch(self, user_batch: List[str]) -> List[Dict[str, Any]]:
        """
        Process EOD snapshots for a batch of users with controlled concurrency.
        """
        semaphore = asyncio.Semaphore(self.max_concurrent)
        
        async def process_single_user_eod(user_id):
            async with semaphore:
                return await self._capture_user_eod_snapshot(user_id)
        
        # Execute all users in batch concurrently
        tasks = [process_single_user_eod(user_id) for user_id in user_batch]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Convert exceptions to error results
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append({
                    'success': False,
                    'user_id': user_batch[i],
                    'error': str(result)
                })
            else:
                processed_results.append(result)
        
        return processed_results
    
    async def _capture_user_eod_snapshot(self, user_id: str) -> Dict[str, Any]:
        """
        Capture end-of-day snapshot for a single user.
        
        Much simpler than reconstruction - just get current portfolio value
        and store as the next point in their historical timeline.
        
        CRITICAL: Uses live price enrichment to ensure accurate EOD values.
        """
        try:
            # Get aggregated holdings with LIVE price enrichment
            supabase = self._get_supabase_client()
            holdings_result = supabase.table('user_aggregated_holdings')\
                .select('*')\
                .eq('user_id', user_id)\
                .execute()
            
            if not holdings_result.data:
                return {
                    'success': False,
                    'user_id': user_id,
                    'error': 'No holdings found'
                }
            
            # Enrich with live prices using production-grade enrichment service
            from utils.portfolio.live_enrichment_service import get_enrichment_service
            from utils.portfolio.aggregated_calculations import calculate_portfolio_value
            
            enrichment_service = get_enrichment_service()
            enriched_holdings = enrichment_service.enrich_holdings(holdings_result.data, user_id)
            
            # Calculate portfolio value from enriched holdings
            portfolio_value = calculate_portfolio_value(enriched_holdings, user_id)
            current_value = portfolio_value.get('raw_value', 0)
            
            if current_value <= 0:
                return {
                    'success': False,
                    'user_id': user_id,
                    'error': 'No portfolio value available'
                }
            
            # Calculate cost basis and gain/loss from enriched holdings
            total_cost_basis = sum(float(h.get('total_cost_basis', 0)) for h in enriched_holdings)
            total_gain_loss = current_value - total_cost_basis
            total_gain_loss_percent = (total_gain_loss / total_cost_basis * 100) if total_cost_basis > 0 else 0
            
            # Get account breakdown from aggregated holdings
            account_breakdown, institution_breakdown = await self._get_account_breakdown(user_id)
            
            # Create EOD snapshot with live-enriched data
            eod_snapshot = EODSnapshot(
                user_id=user_id,
                snapshot_date=datetime.now().date(),
                total_value=current_value,
                total_cost_basis=total_cost_basis,
                total_gain_loss=total_gain_loss,
                total_gain_loss_percent=total_gain_loss_percent,
                account_breakdown=account_breakdown,
                institution_breakdown=institution_breakdown,
                securities_count=len(enriched_holdings),
                data_quality_score=100.0  # Full quality from live prices
            )
            
            # Store EOD snapshot (extends historical timeline)
            await self._store_eod_snapshot(eod_snapshot)
            
            return {
                'success': True,
                'user_id': user_id,
                'portfolio_value': current_value,
                'securities_count': eod_snapshot.securities_count
            }
            
        except Exception as e:
            logger.error(f"Error capturing EOD snapshot for user {user_id}: {e}")
            return {
                'success': False,
                'user_id': user_id,
                'error': str(e)
            }
    
    async def _get_account_breakdown(self, user_id: str) -> Tuple[Dict[str, float], Dict[str, float]]:
        """
        Get account and institution breakdown from aggregated holdings.
        """
        try:
            supabase = self._get_supabase_client()
            
            # Get aggregated holdings with account contributions
            result = supabase.table('user_aggregated_holdings')\
                .select('symbol, total_market_value, account_contributions, institution_breakdown')\
                .eq('user_id', user_id)\
                .execute()
            
            account_breakdown = {}
            institution_breakdown = {}
            
            if result.data:
                for holding in result.data:
                    market_value = float(holding['total_market_value'])
                    
                    # Parse account contributions
                    try:
                        contributions = json.loads(holding['account_contributions']) if holding['account_contributions'] else []
                        for contrib in contributions:
                            account_id = contrib.get('account_id', 'unknown')
                            contrib_value = contrib.get('market_value', 0)
                            account_breakdown[account_id] = account_breakdown.get(account_id, 0) + contrib_value
                    except:
                        pass
                    
                    # Parse institution breakdown
                    try:
                        institutions = json.loads(holding['institution_breakdown']) if holding['institution_breakdown'] else {}
                        for institution in institutions:
                            institution_breakdown[institution] = institution_breakdown.get(institution, 0) + market_value
                    except:
                        # Fallback
                        institution_breakdown['Unknown'] = institution_breakdown.get('Unknown', 0) + market_value
            
            return account_breakdown, institution_breakdown
            
        except Exception as e:
            logger.error(f"Error getting account breakdown for user {user_id}: {e}")
            return {}, {}
    
    async def _store_eod_snapshot(self, snapshot: EODSnapshot):
        """
        Store end-of-day snapshot in portfolio history table.
        
        This extends the user's historical timeline forward by one day.
        """
        try:
            supabase = self._get_supabase_client()
            
            snapshot_data = {
                'user_id': snapshot.user_id,
                'value_date': snapshot.snapshot_date.isoformat(),
                'snapshot_type': 'daily_eod',
                'total_value': snapshot.total_value,
                'total_cost_basis': snapshot.total_cost_basis,
                'total_gain_loss': snapshot.total_gain_loss,
                'total_gain_loss_percent': min(max(snapshot.total_gain_loss_percent, -999.99), 999.99),  # Cap for database
                'account_breakdown': json.dumps(snapshot.account_breakdown),
                'institution_breakdown': json.dumps(snapshot.institution_breakdown),
                'data_source': 'daily_job',
                'price_source': 'plaid_current',
                'data_quality_score': snapshot.data_quality_score,
                'securities_count': snapshot.securities_count
            }
            
            # PRODUCTION-GRADE: Use delete+insert instead of upsert
            # The partitioned table doesn't have a unique constraint we can use with ON CONFLICT
            # First delete any existing daily_eod snapshot for this user/date
            supabase.table('user_portfolio_history')\
                .delete()\
                .eq('user_id', snapshot.user_id)\
                .eq('value_date', snapshot.snapshot_date.isoformat())\
                .eq('snapshot_type', 'daily_eod')\
                .execute()
            
            # Then insert the new snapshot
            supabase.table('user_portfolio_history')\
                .insert(snapshot_data)\
                .execute()
            
            logger.debug(f"ðŸ’¾ Stored EOD snapshot for user {snapshot.user_id}: ${snapshot.total_value:.2f}")
            
        except Exception as e:
            logger.error(f"Error storing EOD snapshot for user {snapshot.user_id}: {e}")
    
    async def _store_collection_metrics(self, result: EODBatchResult):
        """
        Store daily collection metrics for monitoring and optimization.
        """
        try:
            # This could store in a separate metrics table for monitoring
            # For now, just log comprehensive metrics
            
            success_rate = (result.successful_snapshots / result.total_users_processed * 100) if result.total_users_processed > 0 else 0
            avg_processing_time = result.processing_duration_seconds / result.total_users_processed if result.total_users_processed > 0 else 0
            
            metrics = {
                'collection_date': datetime.now().date().isoformat(),
                'total_users_processed': result.total_users_processed,
                'successful_snapshots': result.successful_snapshots,
                'failed_snapshots': result.failed_snapshots,
                'success_rate_percent': success_rate,
                'total_portfolio_value': result.total_portfolio_value,
                'processing_duration_seconds': result.processing_duration_seconds,
                'average_time_per_user': avg_processing_time,
                'api_calls_made': result.api_calls_made,
                'error_count': len(result.errors)
            }
            
            logger.info(f"ðŸ“Š Daily collection metrics: {json.dumps(metrics, indent=2)}")
            
        except Exception as e:
            logger.error(f"Error storing collection metrics: {e}")

class DailyPortfolioScheduler:
    """
    Production scheduler for daily portfolio snapshot collection.
    
    Runs daily at 4 AM EST (after market close) to capture accurate EOD values.
    Handles timezone considerations and market holiday detection.
    """
    
    def __init__(self):
        """Initialize the scheduler."""
        self.snapshot_service = DailyPortfolioSnapshotService()
        self.est_timezone = pytz.timezone('US/Eastern')
        self.target_time = time(4, 0)  # 4:00 AM EST
        self.is_running = False
    
    async def start_daily_scheduler(self):
        """
        Start the production daily scheduler.
        
        Runs continuously, executing daily collection at 4 AM EST.
        """
        if self.is_running:
            logger.warning("Daily scheduler already running")
            return
        
        self.is_running = True
        logger.info("ðŸ“… Starting daily portfolio snapshot scheduler")
        logger.info(f"â° Target time: {self.target_time} EST")
        
        try:
            while self.is_running:
                # Calculate next run time
                next_run = self._calculate_next_run_time()
                wait_seconds = (next_run - datetime.now(self.est_timezone)).total_seconds()
                
                logger.info(f"â° Next EOD collection: {next_run.strftime('%Y-%m-%d %H:%M %Z')} "
                           f"(in {wait_seconds/3600:.1f} hours)")
                
                # Wait until target time
                await asyncio.sleep(wait_seconds)
                
                # PRODUCTION-GRADE: Capture EOD snapshots EVERY DAY (including weekends)
                # This ensures continuous portfolio history charts without gaps
                # Weekend values will be the same as Friday's close (market is closed)
                # but this maintains data continuity for charting purposes
                logger.info(f"ðŸŒ… Starting scheduled EOD collection at {datetime.now(self.est_timezone).strftime('%Y-%m-%d %H:%M %Z')}")
                
                # Execute daily collection
                result = await self.snapshot_service.capture_all_users_eod_snapshots()
                
                logger.info(f"âœ… Scheduled collection complete: {result.successful_snapshots} snapshots, "
                           f"${result.total_portfolio_value:,.2f} total value")
        
        except Exception as e:
            logger.error(f"âŒ Daily scheduler error: {e}")
        finally:
            self.is_running = False
            logger.info("â¹ï¸ Daily portfolio scheduler stopped")
    
    def _calculate_next_run_time(self) -> datetime:
        """
        Calculate the next 4 AM EST run time.
        """
        now_est = datetime.now(self.est_timezone)
        
        # If current time is before 4 AM today, run today
        # Otherwise, run tomorrow
        if now_est.time() < self.target_time:
            next_run_date = now_est.date()
        else:
            next_run_date = now_est.date() + timedelta(days=1)
        
        # Create next run datetime in EST
        next_run = self.est_timezone.localize(
            datetime.combine(next_run_date, self.target_time)
        )
        
        return next_run
    
    def stop_scheduler(self):
        """Stop the daily scheduler."""
        self.is_running = False
        logger.info("ðŸ›‘ Daily scheduler stop requested")

# Global service instances
daily_portfolio_service = DailyPortfolioSnapshotService()
daily_portfolio_scheduler = DailyPortfolioScheduler()

def get_daily_portfolio_service() -> DailyPortfolioSnapshotService:
    """Get the global daily portfolio service instance."""
    return daily_portfolio_service

def get_daily_portfolio_scheduler() -> DailyPortfolioScheduler:
    """Get the global daily portfolio scheduler instance.""" 
    return daily_portfolio_scheduler
