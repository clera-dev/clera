# CS 175 Project Presentation: Memory-Augmented RL for Clera

**Duration:** 2-3 minutes | **Slides:** 7

---

## SLIDE 1: Title Slide

### ON SCREEN:
```
Memory-Augmented Reinforcement Learning 
for Clera's Multi-Agent Investment Advisor

Cristian Mendoza, Delphine Tai-Beauchamp, Agaton Pourshahidi
CS 175 - Reinforcement Learning
```

### SCRIPT:
"Hi everyone! Today we're presenting our final project on applying reinforcement learning to Clera, a multi-agent AI investment advisor we've been building. We're implementing memory-based RL to help our agents learn from past interactions, just like human wealth managers do."

**[5 seconds]**

---

## SLIDE 2: The Problem

### ON SCREEN:
```
THE PROBLEM
âŒ Current AI advisors forget everything
âŒ Can't learn from mistakes  
âŒ No personalization over time

Human wealth managers:
âœ“ Remember past conversations
âœ“ Learn from what works
âœ“ Adapt to each client
```

### SCRIPT:
"Here's the problem: most AI financial advisors, including our current version of Clera, operate completely statelessly. Every conversation starts from scratch. If a user corrects the AI or gives feedback, it's forgotten by the next session. Human wealth managers don't work this wayâ€”they remember your preferences, learn from successful recommendations, and improve over time. That's what we're solving."

**[30 seconds]**

---

## SLIDE 3: Our Solution - RL with LangMem

### ON SCREEN:
```
SOLUTION: Memory-Based Reinforcement Learning

LangMem Framework
â”œâ”€ Semantic Memory: User preferences & successful patterns
â”œâ”€ Episodic Memory: Past successful conversations  
â””â”€ Procedural Memory: Optimized decision-making prompts

User Feedback = Reward Signal
ðŸ‘ +1  |  ðŸ‘Ž -1  |  30-day portfolio performance
```

### SCRIPT:
"Our solution uses LangMem, a memory framework for AI agents, to implement reinforcement learning. We have three memory types: semantic memory stores facts like user preferences and successful strategiesâ€”for example, 'tech stock recommendations during rallies yield 15% better returns.' Episodic memory captures entire successful conversations for behavioral cloning. And procedural memory optimizes our agent prompts based on accumulated feedback. The key insight is that user thumbs up and thumbs down become our reward signals, and portfolio performance over 30 days provides delayed rewards."

**[45 seconds]**

---

## SLIDE 4: How It Works - The RL Loop

### ON SCREEN:
```
THE REINFORCEMENT LEARNING LOOP

1. STATE: User query + retrieved memories + market data
2. ACTION: Agent generates recommendation
3. REWARD: Thumbs up/down (immediate)
           Portfolio performance (delayed)
4. LEARNING: Store experience weighted by reward
5. IMPROVE: Retrieve similar successful patterns for future queries

Experience Replay â†’ Behavioral Cloning â†’ Better Recommendations
```

### SCRIPT:
"Here's how the RL loop works: The state includes the user's query, relevant memories we retrieve, and current market conditions. Our agents take actions by generating investment recommendations. We collect rewards through explicit user feedbackâ€”thumbs up or downâ€”and delayed rewards by measuring portfolio performance versus the S&P 500 after 30 days. The system learns by storing successful interactions with high importance weights and failed approaches with negative weights. Then for future similar queries, we retrieve those successful patterns and mimic themâ€”that's behavioral cloning from experience replay, a core RL technique."

**[50 seconds]**

---

## SLIDE 5: Implementation Details

### ON SCREEN:
```
IMPLEMENTATION (1-2 Weeks)

Technical Stack:
â€¢ LangMem + LangGraph (already integrated)
â€¢ PostgreSQL with vector embeddings (existing)
â€¢ ~850 lines of new code split across team

Data:
â€¢ 500+ real user conversations
â€¢ 100 synthetic bootstrap examples
â€¢ Market data APIs (Financial Modeling Prep, SnapTrade)

We're leveraging existing infrastructure!
```

### SCRIPT:
"Implementation is realistic for our timeline because we're building on existing infrastructure. Clera already uses LangGraph for multi-agent orchestration and PostgreSQL for persistenceâ€”we even explored LangMem before but didn't integrate it. We'll write about 850 lines of code split across the three of us. For data, we'll collect 500+ real conversations with user feedback, plus we're creating 100 synthetic examples to bootstrap the memory before we have real data. The key here is we're not building everything from scratchâ€”we're adding RL capabilities to a working system."

**[35 seconds]**

---

## SLIDE 6: Expected Results & Evaluation

### ON SCREEN:
```
EVALUATION METRICS

ðŸ“Š Memory Growth: Track accumulation over time
ðŸ“ˆ Recommendation Accuracy: Portfolio performance 
   Baseline (Week 1-2) â†’ Learned (Week 6-8)
   Target: +5-10% alpha improvement
   
ðŸ˜Š User Satisfaction: Thumbs-up rate
   Baseline: 65% â†’ Target: 80%+
   
ðŸŽ¯ Memory Relevance: >80% retrieval accuracy

A/B Testing: Baseline Clera vs Memory-Augmented Clera
```

### SCRIPT:
"For evaluation, we're measuring four key metrics. First, memory accumulationâ€”we expect exponential growth early that stabilizes around 200 interactions. Second, recommendation accuracy by comparing 30-day portfolio performance in early weeks versus later weeksâ€”we're targeting 5 to 10 percent alpha improvement over baseline. Third, user satisfaction through thumbs-up rates, going from 65% baseline to 80%+. And fourth, memory retrieval relevanceâ€”making sure the memories we pull are actually useful. We'll run A/B tests with 50 identical queries through both the baseline system and memory-augmented system, and use paired t-tests for statistical significance."

**[45 seconds]**

---

## SLIDE 7: Why This Matters

### ON SCREEN:
```
IMPACT

For CS 175:
âœ“ Real-world RL application
âœ“ Experience replay + behavioral cloning
âœ“ Immediate & delayed rewards
âœ“ Measurable learning over time

For Clera Users:
âœ“ Personalized advice that improves with each interaction
âœ“ AI that learns from mistakes
âœ“ Wealth management that remembers you

This is RL applied to modern LLM agent systems.
```

### SCRIPT:
"So why does this matter? From an academic perspective, we're applying real RL principlesâ€”experience replay, behavioral cloning, reward-based learningâ€”to a modern LLM agent system. This is how RL works in practice with large language models: memory-based learning instead of expensive model retraining. And from a product perspective, we're building something users actually want: an AI financial advisor that remembers them, learns from their feedback, and improves over time. It's reinforcement learning meets real-world AI agents. Happy to take questions!"

**[35 seconds]**

---

## SLIDE 8: Thank You / Questions

### ON SCREEN:
```
Thank You!

Questions?

Cristian Mendoza, Delphine Tai-Beauchamp, Agaton Pourshahidi
cfmendo1@uci.edu
```

### SCRIPT:
"Thank you! We're happy to answer any questions."

**[5 seconds + Q&A]**

---

## PRESENTATION TIPS:

### Before You Start:
- **Practice the transitions** between slidesâ€”they should feel natural
- **Time yourself**: Aim for 2:30-3:00 total
- **Have one person present** or split smoothly (e.g., Cristian: Slides 1-4, Delphine: Slide 5, Agaton: Slides 6-7)

### During Presentation:
- **Slide 2 (Problem)**: This is your hookâ€”make it relatable
- **Slide 3-4 (Solution/How)**: This is where you sell the RL connectionâ€”emphasize "experience replay," "behavioral cloning," "reward signals"
- **Slide 5 (Implementation)**: Show it's realistic and achievable
- **Slide 6 (Results)**: Show you've thought through evaluation rigorously
- **Slide 7 (Impact)**: End strongâ€”connect back to RL principles

### If Asked About RL Connection:
"We're using memory-based RLâ€”storing experiences weighted by rewards, then retrieving and mimicking successful past behaviors. This is experience replay combined with behavioral cloning, which are established RL techniques. Instead of updating neural network weights, we're updating a memory store, but the learning principle is the same: optimize policy based on reward feedback."

### If Asked About Timeline:
"We already have the infrastructureâ€”LangGraph, PostgreSQL, even explored LangMem before. We're adding RL on top of existing functionality. The 850 lines across three people is about 280 lines each, which is very doable in 1-2 weeks."

---

## TOTAL TIME BREAKDOWN:
- Slide 1: 5s
- Slide 2: 30s  
- Slide 3: 45s
- Slide 4: 50s
- Slide 5: 35s
- Slide 6: 45s
- Slide 7: 35s
- Slide 8: 5s

**Total: ~3:30 minutes** (leaves 1:30 for Q&A if needed)



